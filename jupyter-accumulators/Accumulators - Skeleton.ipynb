{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Accumulators\n",
    "\n",
    "Spark accumulators are *distributed counters* which allow you to increment a global counter in Python UDFs. This is useful for counting certain events or cases, which are not directly part of your data processing. Best example would be to count broken records.\n",
    "\n",
    "## Weather Data Example\n",
    "We will use the weather measurement data again as an example. Instead of using the Spark functions to extract the measurement information, we will write a Python UDF instead. Although this would not be required in our example, this approach might actually be useful in different scenarios. Even the weather data set contains more information which is at non-fixed locations and could not be extracted using simple Spark/SQL string functions.\n",
    "\n",
    "This example will show to use accumulators to count records. For example this might be useful to count broken records in other examples (weather data does not have broken records, though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if not 'spark' in locals():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\",\"24G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load Data\n",
    "\n",
    "As we will not use the previous extraction, we simply load a single year as text data. In the next section we will apply a Python UDF to extract the desired information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageLocation = \"s3://dimajix-training/data/weather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "raw_weather = spark.read.text(storageLocation + \"/2003\").withColumn(\"year\", f.lit(2003))\n",
    "raw_weather.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Extract Weather\n",
    "\n",
    "We will now create and test a simple Python UDF for extracting the weather data. In the next section we will improve that function for counting invalid USAF and WBAN codes. But step by step..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define Python UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"usaf\", StringType()),\n",
    "    StructField(\"wban\", StringType()),\n",
    "    StructField(\"air_temperature\", FloatType()),\n",
    "    StructField(\"air_temperature_qual\", IntegerType()),\n",
    "])\n",
    "\n",
    "@f.udf(schema)\n",
    "def extract_weather(row):\n",
    "    usaf = row[4:10]\n",
    "    wban = row[10:15]\n",
    "    air_temperature = float(row[87:92])/10\n",
    "    air_temperature_qual = int(row[92])\n",
    "    return (usaf, wban, air_temperature, air_temperature_qual)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Use Python UDF\n",
    "\n",
    "Now we can apply the Python UDF `extract_weather` to process our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = # YOUR CODE HERE\n",
    "result.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Schema\n",
    "\n",
    "Since the UDF returned multiple columns, we now have a nested schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Use Accumulators\n",
    "\n",
    "As we just said, we want to improve the Python UDF to count certain important events. For example you might be interested at how many records are broken (none in our data set). We chose a different example: We want to count the number of invalid USAF and WBAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_processed = # YOUR CODE HERE\n",
    "invalid_usaf = # YOUR CODE HERE\n",
    "invalid_wban = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Increment accumulators\n",
    "\n",
    "Now we need to adopt our Python UDF to increment accumulators on specific events. We want to increment each of the accumulators whenever we process an invalid usaf and/or wban."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"usaf\", StringType()),\n",
    "    StructField(\"wban\", StringType()),\n",
    "    StructField(\"air_temperature\", FloatType()),\n",
    "    StructField(\"air_temperature_qual\", IntegerType()),\n",
    "])\n",
    "\n",
    "@f.udf(schema)\n",
    "def extract_weather(row):\n",
    "    usaf = row[4:10]\n",
    "    wban = row[10:15]\n",
    "    air_temperature = float(row[87:92])/10\n",
    "    air_temperature_qual = int(row[92])\n",
    "    \n",
    "    # Increment accumulators\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return (usaf, wban, air_temperature, air_temperature_qual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Execute Query\n",
    "\n",
    "Now we can use the modified UDF and check if the accumulators are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = # YOUR CODE HERE\n",
    "result.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"records_processed=\" + str(records_processed.value))\n",
    "print(\"invalid_usaf=\" + str(invalid_usaf.value))\n",
    "print(\"invalid_wban=\" + str(invalid_wban.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Force Accumulator Updates\n",
    "\n",
    "Surprisingly the counters are not increased. Apparently Spark will only propagate accumulators for fully processed partitions. We can enforce this by moving the `limit` operation up as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_result = # YOUR CODE HERE\n",
    "limit_result.show()\n",
    "\n",
    "print(\"records_processed=\" + str(records_processed.value))\n",
    "print(\"invalid_usaf=\" + str(invalid_usaf.value))\n",
    "print(\"invalid_wban=\" + str(invalid_wban.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running on all records using `count`\n",
    "\n",
    "Now let's try to execute the UDF for every record. The method `count()` should do the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = # YOUR CODE HERE\n",
    "df.show()\n",
    "\n",
    "print(\"records_processed=\" + str(records_processed.value))\n",
    "print(\"invalid_usaf=\" + str(invalid_usaf.value))\n",
    "print(\"invalid_wban=\" + str(invalid_wban.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the execution plan explains that the Spark optimizer actually removed the Python UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running on all records using a `filter` condition\n",
    "\n",
    "Since that didn't work either, because Spark is too clever, let's force the execution by adding a filter condition which requries the UDF to be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "print(\"records_processed=\" + str(records_processed.value))\n",
    "print(\"invalid_usaf=\" + str(invalid_usaf.value))\n",
    "print(\"invalid_wban=\" + str(invalid_wban.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running again, watching the accumulators\n",
    "\n",
    "Accumulators won't be reset automatically between query executions. This underlines that accumulators are an *execution metric* and not a purely data dependent result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result.filter(result[\"measurement.wban\"] != '123').select(f.count(\"*\"))\n",
    "df.show()\n",
    "\n",
    "print(\"records_processed=\" + str(records_processed.value))\n",
    "print(\"invalid_usaf=\" + str(invalid_usaf.value))\n",
    "print(\"invalid_wban=\" + str(invalid_wban.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset Counter\n",
    "\n",
    "You can also reset counters by simply assign them a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result.filter(result[\"measurement.wban\"] != '123').select(f.count(\"*\"))\n",
    "df.show()\n",
    "\n",
    "print(\"records_processed=\" + str(records_processed.value))\n",
    "print(\"invalid_usaf=\" + str(invalid_usaf.value))\n",
    "print(\"invalid_wban=\" + str(invalid_wban.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Afterthought\n",
    "\n",
    "It is important to understand that Spark accumulators actually count how often a specific event was triggered in our Python UDF. Since Spark might evaluate certain code paths multiple times (for example in cases of node failures or in cases when the execution plan executes a certain step multiple times). Therefore accumulators cannot and therefore should not be used for generating statistics over the data itself. But they can be used to understand which code paths have been used more often than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
